import argparse

parser = argparse.ArgumentParser()
parser.add_argument('--env', default="tmaze-v0", help="Environment name")
parser.add_argument('--algo', default="PPO", help="RL learning algorithm: DQN, PPO, RecurrentPPO")
parser.add_argument('--stack_type', default="no_stack", help="no_stack, framestack, adaptive, demir")
parser.add_argument('--arch', choices=['cnn', 'mlp', 'transformer', 'lstm'], default='mlp', help="Policy architecture")
parser.add_argument("--with_cnn", help="Use CNN in feature extractor.", action='store_true', default=False)
parser.add_argument("--single_head", help="Type of memory action output neurons.", action='store_true', default=False)
parser.add_argument("--fully_obs", help="Fully observable if available. E.g tmaze, xormaze, fetch", action='store_true', default=False)
parser.add_argument("--no_reset", help="Continual with no reset if available. E.g tmaze, xormaze, fetch", action='store_true', default=False)
parser.add_argument('--cube_cam', default="orthographic", help="full, face, orthographic")
parser.add_argument('--scramble_steps', type=int, default=5, help="Scramble steps for cube env")
parser.add_argument('--maze_length', type=int, default=5, help="Maze length for tmaze and xormaze, or Grid size for MiniGrid")
parser.add_argument("--random_length", help="Random maze lengths per episode sampled between min (2) and max (maze_length)", action='store_true', default=False)
parser.add_argument('--active', action='store_true', default=False, help="Active tmaze mode")
parser.add_argument('--visible_goal_steps', type=int, default=2, help="Number of steps where the environment goal is visible in GCRL tasks")
parser.add_argument('--max_episode_steps', type=int, default=50, help="Max number of steps per episode")
parser.add_argument('--num_stack', type=int, default=1, help="Memory length (sequence length)")
parser.add_argument('--maxiter', type=int, default=int(1e6), help="Max training timesteps")
parser.add_argument('--features_dim', type=int, default=256, help="Input dim of policy layer")
parser.add_argument('--hidden_size', type=int, default=128, help="Hidden dim of memory architecture layer")
parser.add_argument('--num_layers', type=int, default=2, help="Feauture extractor layers")
parser.add_argument('--n_steps', type=int, default=128, help="Number of steps per update")
parser.add_argument('--batch_size', type=int, default=128, help="Batch size")
parser.add_argument('--n_envs', type=int, default=1, help="Number of envs/processes")
parser.add_argument('--seed', type=int, default=None, help="Random seed / run id")
parser.add_argument('--path', default="./data/", help="Save path for logs and models")
parser.add_argument('--load_path', default=None, help="Load path for models")
parser.add_argument('--device', default="cuda", help="Device for Pytorch")
parser.add_argument('--render_mode', default='rgb_array', help="Render mode")
parser.add_argument("--render_2d", help="Render Rubik's cube in 2D or 3D when render_mode=human", action='store_true', default=False)
parser.add_argument("--agent_view", help="Show RGB agent observations/stack when using manual_control.py", action='store_true', default=False)
parser.add_argument("--save_video", help="Save evaluation video when using enjoy.py", action='store_true', default=False)
parser.add_argument("--enjoy_step_by_step", help="Step through policy evaluation step-by-step when using enjoy.py", action='store_true', default=False)
